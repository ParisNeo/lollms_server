# lollms_server Configuration Example

[server]
host = "0.0.0.0"
port = 9600
# List of allowed origins for CORS (Cross-Origin Resource Sharing).
# Needed for web UIs accessing the API from different domains/ports.
# Use ["*"] to allow all (less secure). Default includes common local dev ports.
allowed_origins = [
    "http://localhost",
    "http://localhost:8000", # Common dev port
    "http://localhost:5173", # Default Vite port
    "http://127.0.0.1",
    "http://127.0.0.1:8000",
    "http://127.0.0.1:5173",
    "null" # For file:// access - REMOVE FOR PRODUCTION if not needed
]

[logging]
# Logging level. Options: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
log_level = "INFO"
# Logging level as integer. 0:NOTSET, 10:DEBUG, 20:INFO, 30:WARNING, 40:ERROR, 50:CRITICAL
level = 20

[paths]
# Absolute or relative paths from the server's execution directory (where config.toml is)
# Your custom personalities, bindings, functions go here.
personalities_folder = "personal_personalities/"
bindings_folder = "personal_bindings/"
functions_folder = "personal_functions/"
# Base folder for models (some bindings might look here, e.g., for local GGUF files)
# Subfolders like ttt/, tti/, ttv/, ttm/ are expected by some components.
models_folder = "models/"
# Paths to the built-in examples provided with lollms_server.
example_personalities_folder = "zoos/personalities/"
example_bindings_folder = "zoos/bindings/"
example_functions_folder = "zoos/functions/"

[security]
# List of allowed API keys. Clients must send one of these in the X-API-Key header.
# Generate strong random keys for production. KEEP THIS FILE SECURE.
# Example: allowed_api_keys = ["your-secret-key-1", "another-secure-key"]
allowed_api_keys = ["user1_key_abc123"] # Replace with your actual keys

[defaults]
# Default bindings and models to use if not specified in the request or by personality.
# The key (e.g., "default_ollama") is the logical name of an instance defined below in [bindings].
# The model name should be recognized by that specific binding.

# Text-to-Text (TTT)
ttt_binding = "default_ollama" # Logical name of the binding instance defined below
ttt_model = "mistral:latest"    # Default model for the binding

# Text-to-Image (TTI)
tti_binding = "my_dalle_binding"
tti_model = "dall-e-3"

# Text-to-Video (TTV) - Placeholder
ttv_binding = "default_ttv_dummy"
ttv_model = "dummy_video_gen"

# Text-to-Music (TTM) - Placeholder
ttm_binding = "default_ttm_dummy"
ttm_model = "dummy_music_gen"

# Default generation parameters if not overridden by personality or request
default_context_size = 4096     # Example: Default context window (informational)
default_max_output_tokens = 1024 # Example: Default max generation length (maps to max_tokens)

[webui]
# Enable serving the built-in Vue.js web UI.
# Requires building the UI first (cd webui && npm install && npm run build)
# The UI will be served from the root path ('/').
enable_ui = false

[bindings]
# Define specific binding instances here.
# Each key (e.g., "default_ollama") is a unique logical name for this binding setup.
# `type` refers to the Python module/class name in the bindings folder (e.g., `ollama_binding`).
# Other keys are configuration specific to that binding type.

# --- Ollama Example ---
[bindings.default_ollama]
type = "ollama_binding" # Matches filename ollama_binding.py
host = "http://localhost:11434" # Address of your running Ollama server

# --- OpenAI Example ---
[bindings.default_openai]
type = "openai_binding" # Matches filename openai_binding.py
# Provide API Key here OR set OPENAI_API_KEY environment variable
api_key = "YOUR_OPENAI_API_KEY_HERE"
# base_url = "OPTIONAL_OPENAI_COMPATIBLE_ENDPOINT" # e.g., for Groq, Together.ai, Azure

# --- DALL-E Example (uses OpenAI key) ---
[bindings.my_dalle_binding]
type = "dalle_binding"          # Matches the type_name in dalle_binding.py
# Provide API Key here OR set OPENAI_API_KEY environment variable
api_key = "YOUR_OPENAI_API_KEY_HERE"
# Optional overrides for defaults:
model = "dall-e-3"              # Explicitly set model if desired ('dall-e-3' or 'dall-e-2')
# default_size = "1792x1024"    # Optional: override default size
# default_quality = "hd"        # Optional: override default quality ('standard', 'hd')
# default_style = "natural"     # Optional: override default style ('vivid', 'natural')

# --- Gemini Example ---
# [bindings.default_gemini]
# type = "gemini_binding" # Matches filename gemini_binding.py
# # Provide API Key here OR set GOOGLE_API_KEY environment variable
# google_api_key = "YOUR_GOOGLE_API_KEY_HERE"
# # Optional: Control automatic detection of model limits
# auto_detect_limits = true # Default: true. If false, uses manual values below.
# # Optional: Manual overrides if auto_detect_limits is false
# # ctx_size = 8192
# # max_output_tokens = 4096
# # Optional: Override default safety settings (BLOCK_NONE, BLOCK_ONLY_HIGH, BLOCK_MEDIUM_AND_ABOVE, BLOCK_LOW_AND_ABOVE)
# # safety_setting_harassment = "BLOCK_NONE"
# # safety_setting_hate_speech = "BLOCK_MEDIUM_AND_ABOVE"
# # safety_setting_sexually_explicit = "BLOCK_MEDIUM_AND_ABOVE"
# # safety_setting_dangerous_content = "BLOCK_MEDIUM_AND_ABOVE"

#[bindings.local_llamacpp]
#type = "llamacpp_binding"    # Matches llamacpp_binding.py's binding_type_name
#models_folder  = "C:/path/to/your/models folder" # REQUIRED: Set the actual path! Use forward slashes or escape backslashes (\\) on Windows.
#n_gpu_layers = -1              # Use maximum available GPU layers
#n_ctx = 4096                   # Set context size
# Optional overrides:
# threads = 8
# additional_args = {"--mlock": true}

# --- Dummy Binding Examples (for testing) ---
[bindings.default_tti_dummy]
type = "dummy_binding"
mode = "tti" # Custom config for the dummy binding
delay = 0.2

[bindings.default_ttv_dummy]
type = "dummy_binding"
mode = "ttv"
delay = 1.0

[bindings.default_ttm_dummy]
type = "dummy_binding"
mode = "ttm"
delay = 0.8

[resource_manager]
# Strategy for managing access to potentially limited resources (like GPU VRAM).
# 'semaphore': Allows up to 'gpu_limit' concurrent tasks requiring the resource.
# 'simple_lock': Allows only 1 task at a time requiring the resource.
# 'none': No locking (bindings manage resources independently).
gpu_strategy = "semaphore"
gpu_limit = 1 # Max number of concurrent "heavy" tasks (e.g., model loads or generations on GPU)
queue_timeout = 120 # Seconds to wait in queue for a resource before failing request

[personalities_config]
# Optional section to override settings for specific personalities.
# If a personality folder exists but isn't listed here, default settings apply.
# The key MUST match the personality's folder name.

# Example: Disable the potentially dangerous python executor by default
# python_builder_executor = { enabled = false, max_execution_retries = 0 }

# Example: Explicitly enable another one (redundant unless default changes)
# lollms = { enabled = true }

# Example: Personality folder 'experimental_rag' exists but is disabled here
# experimental_rag = { enabled = false }
