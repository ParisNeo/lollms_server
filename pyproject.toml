[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "lollms_server"
version = "0.1.0"
authors = [
    { name="ParisNeo", email="your_email@example.com" }, # Replace with your email
]
description = "A multi-modal generation server compatible with LOLLMS personalities."
readme = "README.md"
license = { file="LICENSE" }
requires-python = ">=3.9"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: Apache Software License",
    "Operating System :: OS Independent",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Communications :: Chat",
    "Development Status :: 3 - Alpha", # Start with Alpha or Beta
]
dependencies = [
    "fastapi>=0.100.0",
    "uvicorn[standard]>=0.23.0",
    "pydantic>=2.0.0",
    "toml>=0.10.0",
    "python-dotenv>=1.0.0",
    "aiofiles>=23.1.0",
    "pyyaml>=6.0",
]

[project.urls]
Homepage = "https://github.com/ParisNeo/lollms_server" # Replace with actual URL later
Bug Tracker = "https://github.com/ParisNeo/lollms_server/issues" # Replace

# Optional: Define extras for different bindings
[project.optional-dependencies]
openai = ["openai>=1.0.0"]
ollama = ["ollama>=0.1.7"]
# huggingface = ["transformers", "torch", "diffusers", "accelerate"] # Example
# llamacpp = ["llama-cpp-python"] # Example

dev = [
    "pytest",
    "pytest-asyncio",
    "httpx", # For testing FastAPI endpoints
    "black",
    "ruff", # Linter
]

# Optional: Define entry points if you want a command-line script
# [project.scripts]
# lollms-server = "lollms_server.main:run_cli" # Example if you add CLI arg parsing
